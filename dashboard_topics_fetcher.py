#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º —Å –¥–∞—à–±–æ—Ä–¥–∞ Macrocosm
–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∏—Ö —Å Dynamic Desirability —Ç–µ–º–∞–º–∏
"""

import asyncio
import aiohttp
import json
import re
from datetime import datetime
from typing import List, Dict, Set

# URL –¥–∞—à–±–æ—Ä–¥–∞ Macrocosm
DASHBOARD_URL = "https://sn13-dashboard.api.macrocosmos.ai/"

async def fetch_dashboard_topics() -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ç—Ä–µ–Ω–¥–æ–≤—ã–µ —Ç–µ–º—ã —Å –¥–∞—à–±–æ—Ä–¥–∞ Macrocosm"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(DASHBOARD_URL, timeout=30) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # –ò—â–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–º—ã/—Ö—ç—à—Ç–µ–≥–∏ –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                    topics = set()
                    
                    # –ü–æ–∏—Å–∫ —Ö—ç—à—Ç–µ–≥–æ–≤
                    hashtag_pattern = r'#([a-zA-Z0-9_]+)'
                    hashtags = re.findall(hashtag_pattern, html_content)
                    for hashtag in hashtags:
                        if len(hashtag) > 2:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                            topics.add(f"#{hashtag.lower()}")
                    
                    # –ü–æ–∏—Å–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
                    keyword_patterns = [
                        r'\b(AI|artificial intelligence|machine learning|blockchain|crypto|bitcoin|ethereum|defi|web3|nft)\b',
                        r'\b(tesla|spacex|neuralink|openai|chatgpt|claude|gemini)\b',
                        r'\b(trump|harris|biden|politics|election|ukraine|israel|gaza)\b',
                        r'\b(stock|market|trading|investment|economy|inflation)\b',
                        r'\b(climate|environment|renewable|energy|sustainability)\b'
                    ]
                    
                    for pattern in keyword_patterns:
                        matches = re.findall(pattern, html_content, re.IGNORECASE)
                        for match in matches:
                            topics.add(match.lower())
                    
                    return list(topics)[:10]  # –¢–æ–ø 10 —Ç–µ–º
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–∞: HTTP {response.status}")
                    return []
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –¥–∞—à–±–æ—Ä–¥—É: {e}")
        return []

def get_gravity_topics() -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º—ã –∏–∑ Dynamic Desirability"""
    try:
        with open('dynamic_desirability/total.json', 'r') as f:
            gravity_data = json.load(f)
        
        topics = []
        for item in gravity_data:
            if item.get('weight', 0) >= 0.8:  # –¢–æ–ª—å–∫–æ —Ç–µ–º—ã —Å –≤—ã—Å–æ–∫–∏–º –≤–µ—Å–æ–º
                label = item['params'].get('label', '')
                if label:
                    # –î–ª—è Reddit —É–±–∏—Ä–∞–µ–º r/
                    if label.startswith('r/'):
                        topics.append(label[2:])
                    # –î–ª—è X —É–±–∏—Ä–∞–µ–º #
                    elif label.startswith('#'):
                        topics.append(label[1:])
                    else:
                        topics.append(label)
        
        return topics
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è gravity —Ç–µ–º: {e}")
        return []

def create_combined_keywords(dashboard_topics: List[str], gravity_topics: List[str]) -> List[str]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–º—ã —Å –¥–∞—à–±–æ—Ä–¥–∞ –∏ gravity, –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"""
    
    combined = set()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–º—ã —Å –¥–∞—à–±–æ—Ä–¥–∞
    for topic in dashboard_topics:
        combined.add(topic.lower().replace('#', ''))
    
    # –î–æ–±–∞–≤–ª—è–µ–º gravity —Ç–µ–º—ã
    for topic in gravity_topics:
        combined.add(topic.lower())
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    enhanced_topics = set()
    
    for topic in combined:
        enhanced_topics.add(topic)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        if topic in ['bitcoin', 'btc']:
            enhanced_topics.update(['bitcoin', 'btc', 'cryptocurrency', 'crypto'])
        elif topic in ['ai', 'artificial intelligence']:
            enhanced_topics.update(['ai', 'artificialintelligence', 'machinelearning', 'deeplearning'])
        elif topic in ['tesla', 'ev', 'electric']:
            enhanced_topics.update(['tesla', 'electricvehicle', 'ev', 'electriccar'])
        elif topic in ['climate', 'environment']:
            enhanced_topics.update(['climatechange', 'environment', 'sustainability', 'greenenergy'])
        elif topic in ['defi', 'decentralizedfinance']:
            enhanced_topics.update(['defi', 'decentralizedfinance', 'blockchain'])
        elif topic in ['trump', 'politics']:
            enhanced_topics.update(['trump', 'politics', 'election', 'harris'])
        elif topic in ['ukraine', 'war']:
            enhanced_topics.update(['ukraine', 'russia', 'war', 'conflict'])
        elif topic in ['israel', 'gaza']:
            enhanced_topics.update(['israel', 'gaza', 'palestine', 'conflict'])
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    final_topics = []
    for topic in sorted(enhanced_topics):
        if len(topic) >= 3:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            final_topics.append(topic)
    
    return final_topics[:25]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 25 —Ç–µ–º

async def update_scraping_config():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ —Å –Ω–æ–≤—ã–º–∏ —Ç–µ–º–∞–º–∏"""
    
    print("üîç –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ç–µ–º —Å –¥–∞—à–±–æ—Ä–¥–∞ Macrocosm...")
    dashboard_topics = await fetch_dashboard_topics()
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º —Å –¥–∞—à–±–æ—Ä–¥–∞: {len(dashboard_topics)}")
    
    print("üéØ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–º –∏–∑ Dynamic Desirability...")
    gravity_topics = get_gravity_topics()
    print(f"‚öñÔ∏è –ù–∞–π–¥–µ–Ω–æ gravity —Ç–µ–º: {len(gravity_topics)}")
    
    print("üîÄ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–µ–º...")
    combined_keywords = create_combined_keywords(dashboard_topics, gravity_topics)
    
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ ({len(combined_keywords)} —Ç–µ–º):")
    for i, keyword in enumerate(combined_keywords, 1):
        print(f"  {i:2d}. {keyword}")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º config.json –¥–ª—è twikit_scraper
    config_path = "config.json"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        config['search_keywords'] = combined_keywords
        
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω {config_path}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è {config_path}: {e}")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º scraping_config_gravity.json –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–∞–π–Ω–µ—Ä–∞
    scraping_config_path = "scraping_config_gravity.json"
    try:
        with open(scraping_config_path, 'r', encoding='utf-8') as f:
            scraping_config = json.load(f)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Twitter labels
        for scraper in scraping_config.get('scrapers', []):
            if scraper.get('provider') == 'X.microworlds':
                scraper['labels_to_scrape'] = combined_keywords
            elif scraper.get('provider') == 'Reddit.custom':
                # –î–ª—è Reddit –∏—Å–ø–æ–ª—å–∑—É–µ–º r/ –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–º
                reddit_labels = []
                for keyword in combined_keywords:
                    if keyword in ['bitcoin', 'cryptocurrency', 'politics', 'worldnews', 'technology']:
                        reddit_labels.append(f"r/{keyword}")
                    else:
                        reddit_labels.append(keyword)
                scraper['labels_to_scrape'] = reddit_labels[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è Reddit
        
        with open(scraping_config_path, 'w', encoding='utf-8') as f:
            json.dump(scraping_config, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω {scraping_config_path}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è {scraping_config_path}: {e}")
    
    print(f"\nüéØ **–ò–¢–û–ì:** –°–æ–∑–¥–∞–Ω —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ {len(combined_keywords)} —Ç–µ–º")
    print("üìà –í–∫–ª—é—á–∞–µ—Ç: Dashboard trends + Gravity topics + —Å–∏–Ω–æ–Ω–∏–º—ã")
    print("üöÄ –ì–æ—Ç–æ–≤–æ –¥–ª—è –¥–µ–ø–ª–æ—è –Ω–∞ VPS!")

if __name__ == "__main__":
    asyncio.run(update_scraping_config())
